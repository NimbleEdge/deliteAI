/*
 * SPDX-FileCopyrightText: (C) 2025 DeliteAI Authors
 *
 * SPDX-License-Identifier: Apache-2.0
 */

#pragma once

#include "base_llm_executor.hpp"
#include "char_stream.hpp"
#include "ne_fwd.hpp"
#include "rigtorp/SPSCQueue.h"

using Queue = rigtorp::SPSCQueue<char>;

/**
 * @class GeminiNanoExecutor
 * @brief Executor class responsible for running inference for Gemini model.
 *
 * This class extends BaseLLMExecutor to integrate with Gemini for local LLM inference.
 * It manages internal inference threads, token streams, and provides prompt execution
 * functionality.
 */
class GeminiNanoExecutor : public BaseLLMExecutor {
  static std::shared_ptr<Queue> _internalQueue;

  static std::mutex _mutex; /**< Mutex used to guard shared state during prompt execution. */

  std::shared_ptr<Task> _task =
      nullptr; /**< Store task so we can add charStreamFillJob to it when it's created */

  std::string _context; /**< Stores context in memory if provided. */

 public:
  /**
   * @brief Constructor for GeminiNanoExecutor.
   *
   * @param task Task used to orchestrate llm generation with delitepy script.
   * @param commandCenter
   */

  GeminiNanoExecutor(std::shared_ptr<Task> task, CommandCenter* commandCenter);

  /**
   * @brief Executes a prompt and returns a character stream with generated output.
   *
   * @param prompt The prompt to be executed.
   * @return Shared pointer to a CharStream containing the response.
   */
  std::shared_ptr<CharStream> run_prompt(const std::string& prompt) override;

  /**
   * @brief Add a prompt to the inference pipeline.
   *
   * @param prompt Input prompt string.
   */
  void add_prompt(const std::string& prompt) override;

  /**
   * @brief Cancel any ongoing inference operation.
   */
  void cancel() override;

  /**
   * @brief Clears model context.
   *
   * @return Shared pointer to a NoneVariable indicating reset completion.
   */
  std::shared_ptr<NoneVariable> clear_context() override;

  /**
   * @brief Called from Kotlin via JNI layer for every token generated by
                                   Gemini model to push them to _internalQueue
   */
  static void push_to_queue(const std::string& text);

  /**
   * @brief Marks the end of the output stream to signal completion of LLM output generation or an
   * error from the executor.
   */
  static void mark_end_of_stream();
};
